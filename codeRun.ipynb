{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 机器学习实战课程\n",
        "\n",
        "> **课程时长**: 1.5小时  \n",
        "> **目标**: 通过MNIST数据集学习MLP、CNN、ResNet，然后进行YOLO目标检测实战\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境准备\n",
        "\n",
        "首先安装和导入必要的库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装必要的库（如果尚未安装）\n",
        "# !pip install torch torchvision matplotlib ultralytics opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 导入所有必要的库 ====================\n",
        "import torch                                    # PyTorch深度学习框架\n",
        "import torch.nn as nn                           # 神经网络模块\n",
        "import torch.optim as optim                     # 优化器\n",
        "import torch.nn.functional as F                 # 常用函数（如ReLU、Softmax等）\n",
        "from torch.utils.data import DataLoader         # 数据加载器\n",
        "import torchvision                              # 计算机视觉工具库\n",
        "import torchvision.transforms as transforms     # 数据预处理/增强\n",
        "from torchvision.datasets import MNIST          # MNIST数据集\n",
        "import matplotlib.pyplot as plt                 # 绑图库\n",
        "import numpy as np                              # 数值计算库\n",
        "\n",
        "# 设置中文显示（如果需要）\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei']    # 用于正常显示中文\n",
        "plt.rcParams['axes.unicode_minus'] = False      # 用于正常显示负号\n",
        "\n",
        "# 检查是否有GPU可用\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'使用设备: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1: MNIST + 全连接网络 (MLP)\n",
        "\n",
        "### 1.1 MNIST数据集介绍\n",
        "\n",
        "- **MNIST**: 手写数字识别数据集，包含0-9共10类数字\n",
        "- **规模**: 60000张训练图片，10000张测试图片\n",
        "- **尺寸**: 28×28像素，灰度图\n",
        "- **地位**: 机器学习的\"Hello World\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 1.2 加载MNIST数据集 ====================\n",
        "\n",
        "# 定义数据预处理流程\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                # 将PIL图像转换为Tensor，并归一化到[0,1]\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # 使用MNIST的均值和标准差进行标准化\n",
        "])\n",
        "\n",
        "# 下载并加载训练集\n",
        "# root: 数据存储路径\n",
        "# train=True: 加载训练集\n",
        "# download=True: 如果数据不存在则自动下载\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# 下载并加载测试集\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# 创建数据加载器\n",
        "# batch_size: 每批次加载64张图片\n",
        "# shuffle: 训练时打乱数据顺序，测试时不打乱\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f'训练集大小: {len(train_dataset)}')\n",
        "print(f'测试集大小: {len(test_dataset)}')\n",
        "print(f'图片尺寸: {train_dataset[0][0].shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 1.3 可视化MNIST样本 ====================\n",
        "\n",
        "# 获取一批数据用于展示\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# 创建3x3的子图\n",
        "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    # 显示图片（需要squeeze去掉通道维度，因为是灰度图）\n",
        "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
        "    ax.set_title(f'标签: {labels[i].item()}', fontsize=12)\n",
        "    ax.axis('off')  # 隐藏坐标轴\n",
        "\n",
        "plt.suptitle('MNIST数据集样本展示', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 全连接网络原理\n",
        "\n",
        "**神经元工作方式**: 输入 → 加权求和 → 激活函数 → 输出\n",
        "\n",
        "**网络结构**:\n",
        "```\n",
        "输入层(784) → 隐藏层(256) → 隐藏层(128) → 输出层(10)\n",
        "```\n",
        "\n",
        "**类比**: 把神经网络比作\"投票系统\"，每个神经元投票，最终多数票决定分类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 1.5 定义全连接网络 (MLP) ====================\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    多层感知机（全连接神经网络）\n",
        "    结构: 784 → 256 → 128 → 10\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        # 第一个全连接层: 输入784维（28*28展平），输出256维\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        \n",
        "        # 第二个全连接层: 输入256维，输出128维\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        \n",
        "        # 第三个全连接层（输出层）: 输入128维，输出10维（对应10个数字类别）\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        \n",
        "        # Dropout层: 训练时随机丢弃20%的神经元，防止过拟合\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        x: 输入张量，形状为 (batch_size, 1, 28, 28)\n",
        "        \"\"\"\n",
        "        # 步骤1: 将28x28的图片展平为784维向量\n",
        "        x = x.view(-1, 28 * 28)  # (batch_size, 784)\n",
        "        \n",
        "        # 步骤2: 第一层 → ReLU激活 → Dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # 步骤3: 第二层 → ReLU激活 → Dropout\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # 步骤4: 输出层（不需要激活函数，交给损失函数处理）\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# 创建模型实例并移动到GPU（如果可用）\n",
        "mlp_model = MLP().to(device)\n",
        "\n",
        "# 打印模型结构\n",
        "print(mlp_model)\n",
        "\n",
        "# 计算模型参数量\n",
        "total_params = sum(p.numel() for p in mlp_model.parameters())\n",
        "print(f'\\nMLP总参数量: {total_params:,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 1.6 定义训练和测试函数 ====================\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    训练函数：执行一个epoch的训练\n",
        "    \n",
        "    参数:\n",
        "        model: 神经网络模型\n",
        "        train_loader: 训练数据加载器\n",
        "        optimizer: 优化器\n",
        "        criterion: 损失函数\n",
        "        device: 运行设备（CPU/GPU）\n",
        "    \n",
        "    返回:\n",
        "        平均训练损失\n",
        "    \"\"\"\n",
        "    model.train()  # 设置为训练模式（启用Dropout等）\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # 将数据移动到指定设备\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # 清零梯度（PyTorch默认会累积梯度）\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 前向传播：计算预测值\n",
        "        output = model(data)\n",
        "        \n",
        "        # 计算损失\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        # 反向传播：计算梯度\n",
        "        loss.backward()\n",
        "        \n",
        "        # 更新权重\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def test(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    测试函数：在测试集上评估模型\n",
        "    \n",
        "    返回:\n",
        "        (平均测试损失, 准确率)\n",
        "    \"\"\"\n",
        "    model.eval()  # 设置为评估模式（禁用Dropout等）\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    # 不计算梯度，节省内存和计算\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # 前向传播\n",
        "            output = model(data)\n",
        "            \n",
        "            # 累加损失\n",
        "            test_loss += criterion(output, target).item()\n",
        "            \n",
        "            # 获取预测结果（概率最大的类别）\n",
        "            pred = output.argmax(dim=1)\n",
        "            \n",
        "            # 统计正确预测的数量\n",
        "            correct += pred.eq(target).sum().item()\n",
        "    \n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    \n",
        "    return test_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 1.7 训练MLP模型 ====================\n",
        "\n",
        "# 定义损失函数：交叉熵损失（适用于多分类问题）\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 定义优化器：Adam优化器\n",
        "# lr: 学习率，控制每次更新的步长\n",
        "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# 训练参数\n",
        "epochs = 5\n",
        "\n",
        "# 记录训练过程\n",
        "mlp_train_losses = []\n",
        "mlp_test_losses = []\n",
        "mlp_accuracies = []\n",
        "\n",
        "print('开始训练MLP...\\n')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # 训练一个epoch\n",
        "    train_loss = train(mlp_model, train_loader, mlp_optimizer, criterion, device)\n",
        "    \n",
        "    # 在测试集上评估\n",
        "    test_loss, accuracy = test(mlp_model, test_loader, criterion, device)\n",
        "    \n",
        "    # 记录结果\n",
        "    mlp_train_losses.append(train_loss)\n",
        "    mlp_test_losses.append(test_loss)\n",
        "    mlp_accuracies.append(accuracy)\n",
        "    \n",
        "    # 打印进度\n",
        "    print(f'Epoch {epoch}/{epochs}:')\n",
        "    print(f'  训练损失: {train_loss:.4f}')\n",
        "    print(f'  测试损失: {test_loss:.4f}')\n",
        "    print(f'  测试准确率: {accuracy:.2f}%\\n')\n",
        "\n",
        "print(f'MLP最终准确率: {mlp_accuracies[-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 1.8 可视化MLP预测结果 ====================\n",
        "\n",
        "# 随机抽取10张测试图片进行预测展示\n",
        "# 每次运行此cell都会随机抽取不同的图片\n",
        "num_samples = 10\n",
        "random_indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
        "\n",
        "# 获取随机抽取的图片和标签\n",
        "test_images = torch.stack([test_dataset[i][0] for i in random_indices]).to(device)\n",
        "test_labels = torch.tensor([test_dataset[i][1] for i in random_indices]).to(device)\n",
        "\n",
        "# 进行预测\n",
        "mlp_model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = mlp_model(test_images).argmax(dim=1)\n",
        "\n",
        "# 可视化预测结果\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = test_images[i].cpu().squeeze()\n",
        "    true_label = test_labels[i].item()\n",
        "    pred_label = predictions[i].item()\n",
        "    \n",
        "    ax.imshow(img, cmap='gray')\n",
        "    \n",
        "    # 预测正确显示绿色，错误显示红色\n",
        "    color = 'green' if pred_label == true_label else 'red'\n",
        "    ax.set_title(f'真实: {true_label}, 预测: {pred_label}', color=color)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('MLP预测结果 (随机抽样)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'本次随机抽取的图片索引: {random_indices}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: MNIST + CNN (卷积神经网络)\n",
        "\n",
        "### 2.1 CNN核心概念\n",
        "\n",
        "**卷积层 (Convolution)**:\n",
        "- 用\"滑动窗口\"扫描图片，提取局部特征\n",
        "- 卷积核就像\"特征探测器\"，检测边缘、纹理等\n",
        "\n",
        "**池化层 (Pooling)**:\n",
        "- 降低分辨率，保留重要信息\n",
        "- 作用: 减少计算量，增强鲁棒性\n",
        "\n",
        "**为什么CNN比全连接好**:\n",
        "- 参数共享: 同一个卷积核扫描整张图\n",
        "- 局部连接: 只关注邻近像素\n",
        "- 平移不变性: 数字在哪都能识别"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 2.2 定义CNN模型 ====================\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    卷积神经网络\n",
        "    结构: Conv1 → Pool → Conv2 → Pool → FC1 → FC2\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        # 第一个卷积层\n",
        "        # in_channels=1: 输入是灰度图（1个通道）\n",
        "        # out_channels=32: 输出32个特征图\n",
        "        # kernel_size=3: 使用3x3的卷积核\n",
        "        # padding=1: 边缘填充1像素，保持尺寸不变\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        \n",
        "        # 第二个卷积层: 32个通道输入，64个通道输出\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        \n",
        "        # 最大池化层: 2x2窗口，步长为2，尺寸减半\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # 全连接层\n",
        "        # 经过两次池化后: 28→14→7，所以输入是 64*7*7\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        \n",
        "        # Dropout防止过拟合\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        x: (batch_size, 1, 28, 28)\n",
        "        \"\"\"\n",
        "        # 第一个卷积块: Conv → ReLU → Pool\n",
        "        # (batch, 1, 28, 28) → (batch, 32, 28, 28) → (batch, 32, 14, 14)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        \n",
        "        # 第二个卷积块: Conv → ReLU → Pool\n",
        "        # (batch, 32, 14, 14) → (batch, 64, 14, 14) → (batch, 64, 7, 7)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        \n",
        "        # 展平: (batch, 64, 7, 7) → (batch, 64*7*7)\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        \n",
        "        # 全连接层\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# 创建CNN模型\n",
        "cnn_model = CNN().to(device)\n",
        "print(cnn_model)\n",
        "\n",
        "# 计算参数量\n",
        "cnn_params = sum(p.numel() for p in cnn_model.parameters())\n",
        "print(f'\\nCNN总参数量: {cnn_params:,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 2.3 训练CNN模型 ====================\n",
        "\n",
        "# 定义优化器\n",
        "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# 记录训练过程\n",
        "cnn_train_losses = []\n",
        "cnn_test_losses = []\n",
        "cnn_accuracies = []\n",
        "\n",
        "print('开始训练CNN...\\n')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(cnn_model, train_loader, cnn_optimizer, criterion, device)\n",
        "    test_loss, accuracy = test(cnn_model, test_loader, criterion, device)\n",
        "    \n",
        "    cnn_train_losses.append(train_loss)\n",
        "    cnn_test_losses.append(test_loss)\n",
        "    cnn_accuracies.append(accuracy)\n",
        "    \n",
        "    print(f'Epoch {epoch}/{epochs}:')\n",
        "    print(f'  训练损失: {train_loss:.4f}')\n",
        "    print(f'  测试损失: {test_loss:.4f}')\n",
        "    print(f'  测试准确率: {accuracy:.2f}%\\n')\n",
        "\n",
        "print(f'CNN最终准确率: {cnn_accuracies[-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 2.4 训练曲线对比 ====================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# 损失曲线\n",
        "axes[0].plot(mlp_train_losses, 'b-o', label='MLP训练损失')\n",
        "axes[0].plot(cnn_train_losses, 'r-s', label='CNN训练损失')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('训练损失对比')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# 准确率曲线\n",
        "axes[1].plot(mlp_accuracies, 'b-o', label='MLP准确率')\n",
        "axes[1].plot(cnn_accuracies, 'r-s', label='CNN准确率')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('测试准确率对比')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 打印对比结果\n",
        "print('\\\\n========== MLP vs CNN 对比 ==========')\n",
        "print(f'模型      | 参数量    | 最终准确率')\n",
        "print(f'MLP       | {sum(p.numel() for p in mlp_model.parameters()):>8,} | {mlp_accuracies[-1]:.2f}%')\n",
        "print(f'CNN       | {sum(p.numel() for p in cnn_model.parameters()):>8,} | {cnn_accuracies[-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: MNIST + ResNet (残差网络)\n",
        "\n",
        ">  **说明**: 这里继续使用MNIST是为了方便与前两个模型对比。实际上ResNet是为ImageNet等复杂任务设计的，用于MNIST有点\"杀鸡用牛刀\"，但能帮助大家快速理解残差连接的核心思想。\n",
        "\n",
        "### 3.1 ResNet核心思想\n",
        "\n",
        "**深度网络的问题**: 网络越深，准确率反而可能下降（梯度消失/爆炸）\n",
        "\n",
        "**残差连接 (Residual Connection)**:\n",
        "- 核心公式: `输出 = F(x) + x`\n",
        "- \"跳跃连接\": 让信息可以绕过某些层直接传递\n",
        "- 类比: 高速公路的\"直通道\"，信息不会堵塞\n",
        "\n",
        "```\n",
        "    ┌─────────────────────┐\n",
        "    │                     │ (跳跃连接)\n",
        "x ──┼──→ [Conv] → [Conv] ─┼──→ (+) → ReLU → 输出\n",
        "    └─────────────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 3.2 定义残差块 ====================\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    残差块 (Residual Block)\n",
        "    \n",
        "    核心思想: 输出 = F(x) + x\n",
        "    其中 F(x) 是两个卷积层的输出\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        \n",
        "        # 第一个卷积层\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)  # 批归一化，加速训练\n",
        "        \n",
        "        # 第二个卷积层\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        # 跳跃连接 (shortcut)\n",
        "        # 如果输入输出通道数不同，或者stride不为1，需要用1x1卷积调整维度\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
        "                         stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        \"\"\"\n",
        "        # 保存输入，用于残差连接\n",
        "        identity = x\n",
        "        \n",
        "        # 主路径: Conv → BN → ReLU → Conv → BN\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        \n",
        "        # 残差连接: 将shortcut(可能经过1x1卷积)加到主路径输出\n",
        "        out += self.shortcut(identity)\n",
        "        \n",
        "        # 最后再经过ReLU激活\n",
        "        out = F.relu(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 3.3 定义简化版ResNet ====================\n",
        "\n",
        "class SimpleResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    简化版ResNet，适用于MNIST\n",
        "    结构: Conv → ResBlock×3 → AvgPool → FC\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(SimpleResNet, self).__init__()\n",
        "        \n",
        "        # 初始卷积层: 将1通道转为16通道\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        \n",
        "        # 三个残差块\n",
        "        self.layer1 = ResidualBlock(16, 16, stride=1)   # 尺寸不变: 28x28\n",
        "        self.layer2 = ResidualBlock(16, 32, stride=2)   # 尺寸减半: 14x14\n",
        "        self.layer3 = ResidualBlock(32, 64, stride=2)   # 尺寸减半: 7x7\n",
        "        \n",
        "        # 全局平均池化: 将7x7压缩为1x1\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        \n",
        "        # 全连接输出层\n",
        "        self.fc = nn.Linear(64, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        \"\"\"\n",
        "        # 初始卷积: (batch, 1, 28, 28) → (batch, 16, 28, 28)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        \n",
        "        # 残差块\n",
        "        x = self.layer1(x)  # (batch, 16, 28, 28)\n",
        "        x = self.layer2(x)  # (batch, 32, 14, 14)\n",
        "        x = self.layer3(x)  # (batch, 64, 7, 7)\n",
        "        \n",
        "        # 全局平均池化: (batch, 64, 7, 7) → (batch, 64, 1, 1)\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        # 展平: (batch, 64, 1, 1) → (batch, 64)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # 全连接: (batch, 64) → (batch, 10)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# 创建ResNet模型\n",
        "resnet_model = SimpleResNet().to(device)\n",
        "print(resnet_model)\n",
        "\n",
        "resnet_params = sum(p.numel() for p in resnet_model.parameters())\n",
        "print(f'\\\\nResNet总参数量: {resnet_params:,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 3.4 训练ResNet模型 ====================\n",
        "\n",
        "resnet_optimizer = optim.Adam(resnet_model.parameters(), lr=0.001)\n",
        "\n",
        "resnet_train_losses = []\n",
        "resnet_test_losses = []\n",
        "resnet_accuracies = []\n",
        "\n",
        "print('开始训练ResNet...\\n')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(resnet_model, train_loader, resnet_optimizer, criterion, device)\n",
        "    test_loss, accuracy = test(resnet_model, test_loader, criterion, device)\n",
        "    \n",
        "    resnet_train_losses.append(train_loss)\n",
        "    resnet_test_losses.append(test_loss)\n",
        "    resnet_accuracies.append(accuracy)\n",
        "    \n",
        "    print(f'Epoch {epoch}/{epochs}:')\n",
        "    print(f'  训练损失: {train_loss:.4f}')\n",
        "    print(f'  测试损失: {test_loss:.4f}')\n",
        "    print(f'  测试准确率: {accuracy:.2f}%\\n')\n",
        "\n",
        "print(f'ResNet最终准确率: {resnet_accuracies[-1]:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 3.5 三种模型对比 ====================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 损失曲线对比\n",
        "axes[0].plot(mlp_train_losses, 'b-o', label='MLP', markersize=8)\n",
        "axes[0].plot(cnn_train_losses, 'r-s', label='CNN', markersize=8)\n",
        "axes[0].plot(resnet_train_losses, 'g-^', label='ResNet', markersize=8)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('训练损失', fontsize=12)\n",
        "axes[0].set_title('训练损失对比', fontsize=14)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 准确率曲线对比\n",
        "axes[1].plot(mlp_accuracies, 'b-o', label='MLP', markersize=8)\n",
        "axes[1].plot(cnn_accuracies, 'r-s', label='CNN', markersize=8)\n",
        "axes[1].plot(resnet_accuracies, 'g-^', label='ResNet', markersize=8)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('测试准确率 (%)', fontsize=12)\n",
        "axes[1].set_title('测试准确率对比', fontsize=14)\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 打印最终对比表格\n",
        "print('\\\\n' + '='*50)\n",
        "print('          MNIST分类模型对比总结')\n",
        "print('='*50)\n",
        "print(f'{\"模型\":<10} | {\"参数量\":>10} | {\"最终准确率\":>10}')\n",
        "print('-'*50)\n",
        "print(f'{\"MLP\":<10} | {sum(p.numel() for p in mlp_model.parameters()):>10,} | {mlp_accuracies[-1]:>9.2f}%')\n",
        "print(f'{\"CNN\":<10} | {sum(p.numel() for p in cnn_model.parameters()):>10,} | {cnn_accuracies[-1]:>9.2f}%')\n",
        "print(f'{\"ResNet\":<10} | {sum(p.numel() for p in resnet_model.parameters()):>10,} | {resnet_accuracies[-1]:>9.2f}%')\n",
        "print('='*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: YOLO目标检测实战\n",
        "\n",
        "### 4.1 YOLO原理简介\n",
        "\n",
        "**目标检测 vs 图像分类**:\n",
        "- 分类: 图片里是什么？→ 一个类别\n",
        "- 检测: 图片里有什么？在哪里？→ 多个物体 + 位置\n",
        "\n",
        "**YOLO核心思想** (You Only Look Once):\n",
        "- 将图片划分成 S×S 网格\n",
        "- 每个网格预测: 边界框(x,y,w,h) + 置信度 + 类别概率\n",
        "- 一次前向传播完成所有预测，速度快！\n",
        "\n",
        "**关键概念**:\n",
        "- **边界框 (Bounding Box)**: 用矩形框定位物体\n",
        "- **置信度 (Confidence)**: 框内有物体的概率\n",
        "- **IoU**: 预测框与真实框的重叠比例\n",
        "- **NMS (非极大值抑制)**: 去除重复的框"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.2 安装和导入YOLO ====================\n",
        "\n",
        "# 安装ultralytics（如果尚未安装）\n",
        "# !pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "print('YOLO环境准备完成！')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 实战一: 图片目标检测"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.3 加载预训练YOLO模型 ====================\n",
        "\n",
        "# 加载YOLOv8n模型（nano版本，最小最快）\n",
        "# 首次运行会自动下载预训练权重\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "print('YOLOv8模型加载成功！')\n",
        "print(f'模型可检测的类别数: {len(model.names)}')\n",
        "print(f'部分类别示例: {list(model.names.values())[:10]}...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.4 下载示例图片并进行检测 ====================\n",
        "\n",
        "def detect_image(image_source, model, conf_threshold=0.25):\n",
        "    \"\"\"\n",
        "    对图片进行目标检测\n",
        "    \n",
        "    参数:\n",
        "        image_source: 图片路径或URL\n",
        "        model: YOLO模型\n",
        "        conf_threshold: 置信度阈值，低于此值的检测结果会被过滤\n",
        "    \"\"\"\n",
        "    # 进行检测\n",
        "    # conf: 置信度阈值\n",
        "    # verbose: 是否打印详细信息\n",
        "    results = model(image_source, conf=conf_threshold, verbose=False)\n",
        "    \n",
        "    # 获取第一张图片的结果\n",
        "    result = results[0]\n",
        "    \n",
        "    # 打印检测结果\n",
        "    print('\\\\n========== 检测结果 ==========')\n",
        "    boxes = result.boxes\n",
        "    if len(boxes) == 0:\n",
        "        print('未检测到任何物体')\n",
        "    else:\n",
        "        for i, box in enumerate(boxes):\n",
        "            # 获取类别名称\n",
        "            cls_id = int(box.cls[0])\n",
        "            cls_name = model.names[cls_id]\n",
        "            \n",
        "            # 获取置信度\n",
        "            confidence = float(box.conf[0])\n",
        "            \n",
        "            # 获取边界框坐标 (x1, y1, x2, y2)\n",
        "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "            \n",
        "            print(f'{i+1}. {cls_name}: {confidence:.2%} | 位置: ({x1:.0f}, {y1:.0f}) - ({x2:.0f}, {y2:.0f})')\n",
        "    \n",
        "    return result\n",
        "\n",
        "# 使用YOLO官方示例图片进行测试\n",
        "# 这是一张包含多个人和公交车的街景图片\n",
        "test_url = 'https://ultralytics.com/images/bus.jpg'\n",
        "\n",
        "print('正在检测示例图片...')\n",
        "result = detect_image(test_url, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.5 可视化检测结果 ====================\n",
        "\n",
        "# 获取带有检测框的图片\n",
        "# result.plot() 返回带有标注的图片数组(BGR格式)\n",
        "annotated_img = result.plot()\n",
        "\n",
        "# 将BGR转换为RGB用于matplotlib显示\n",
        "annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "# 显示结果\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(annotated_img_rgb)\n",
        "plt.title('YOLOv8目标检测结果', fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.6 检测更多示例图片 ====================\n",
        "\n",
        "# 测试另一张图片（斑马图片）\n",
        "test_url2 = 'https://ultralytics.com/images/zidane.jpg'\n",
        "\n",
        "print('正在检测第二张示例图片...')\n",
        "result2 = detect_image(test_url2, model)\n",
        "\n",
        "# 显示结果\n",
        "annotated_img2 = result2.plot()\n",
        "annotated_img2_rgb = cv2.cvtColor(annotated_img2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(annotated_img2_rgb)\n",
        "plt.title('YOLOv8检测结果 - 人物检测', fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 实战二: 视频/摄像头实时检测\n",
        "\n",
        "> **注意**: 以下代码需要在本地环境运行，Jupyter中可能无法直接显示摄像头画面"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.7 视频/摄像头实时检测函数 ====================\n",
        "\n",
        "def detect_video(source=0, model=None, show_fps=True):\n",
        "    \"\"\"\n",
        "    对视频或摄像头进行实时目标检测\n",
        "    \n",
        "    参数:\n",
        "        source: 视频源\n",
        "                - 0: 默认摄像头\n",
        "                - 1, 2...: 其他摄像头\n",
        "                - 'video.mp4': 视频文件路径\n",
        "        model: YOLO模型\n",
        "        show_fps: 是否显示帧率\n",
        "    \n",
        "    操作:\n",
        "        - 按 'q' 键退出\n",
        "        - 按 's' 键保存当前帧\n",
        "    \"\"\"\n",
        "    # 打开视频源\n",
        "    cap = cv2.VideoCapture(source)\n",
        "    \n",
        "    if not cap.isOpened():\n",
        "        print(f'无法打开视频源: {source}')\n",
        "        return\n",
        "    \n",
        "    print('开始实时检测...')\n",
        "    print('按 q 退出 | 按 s 保存当前帧')\n",
        "    \n",
        "    frame_count = 0\n",
        "    \n",
        "    while True:\n",
        "        # 读取一帧\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print('视频结束或无法读取帧')\n",
        "            break\n",
        "        \n",
        "        # 记录开始时间（用于计算FPS）\n",
        "        start_time = cv2.getTickCount()\n",
        "        \n",
        "        # 进行检测\n",
        "        results = model(frame, verbose=False)\n",
        "        \n",
        "        # 获取带标注的帧\n",
        "        annotated_frame = results[0].plot()\n",
        "        \n",
        "        # 计算并显示FPS\n",
        "        if show_fps:\n",
        "            end_time = cv2.getTickCount()\n",
        "            fps = cv2.getTickFrequency() / (end_time - start_time)\n",
        "            cv2.putText(annotated_frame, f'FPS: {fps:.1f}', (10, 30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "        \n",
        "        # 显示结果\n",
        "        cv2.imshow('YOLO Detection', annotated_frame)\n",
        "        \n",
        "        # 键盘控制\n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "        if key == ord('q'):  # 按q退出\n",
        "            break\n",
        "        elif key == ord('s'):  # 按s保存\n",
        "            cv2.imwrite(f'detection_{frame_count}.jpg', annotated_frame)\n",
        "            print(f'已保存: detection_{frame_count}.jpg')\n",
        "        \n",
        "        frame_count += 1\n",
        "    \n",
        "    # 释放资源\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(f'检测结束，共处理 {frame_count} 帧')\n",
        "\n",
        "# 注意: 在Jupyter中运行以下代码可能会有问题\n",
        "# 建议在本地Python环境中运行\n",
        "# \n",
        "# 使用摄像头:\n",
        "# detect_video(source=0, model=model)\n",
        "#\n",
        "# 使用视频文件:\n",
        "# detect_video(source='your_video.mp4', model=model)\n",
        "\n",
        "print('视频检测函数已定义，可在本地环境中调用')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 实战三：模型训练实战\n",
        "\n",
        "前面我们使用的都是**预训练模型**（在大型数据集上训练好的模型）。对于一些特定的场景或特殊需求我们则需要自己训练模型。\n",
        "\n",
        "**接下来你要实现如下的目标**:\n",
        "1. 下载COCO128数据集\n",
        "2. 从零开始训练一个目标检测模型\n",
        "3. 测试训练好的模型效果\n",
        "4. 理解数据集格式和训练流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.5.1 下载COCO128数据集 ====================\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "# COCO128是COCO数据集的迷你版本\n",
        "# Ultralytics会自动下载和管理这个数据集\n",
        "print('准备下载COCO128数据集...')\n",
        "\n",
        "# 创建一个临时模型来触发数据集下载\n",
        "# 使用coco128.yaml配置文件\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# 查看COCO128数据集的配置\n",
        "# Ultralytics将数据集下载到用户目录下的datasets文件夹\n",
        "data_yaml_path = 'coco128.yaml'\n",
        "\n",
        "print('='*50)\n",
        "print('下载完成')\n",
        "print('='*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.5.2 训练模型 ====================\n",
        "\n",
        "print('='*50)\n",
        "print('          开始训练YOLO模型')\n",
        "print('='*50)\n",
        "\n",
        "# 加载预训练的YOLOv8n模型作为起点（迁移学习）\n",
        "# 这比从零开始训练要快得多，效果也更好\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# 开始训练\n",
        "results = model.train(\n",
        "    data='coco128.yaml',      # 使用COCO128数据集\n",
        "    epochs=10,                 # 训练10轮（演示用，实际项目建议至少50-100轮）\n",
        "    imgsz=640,                 # 输入图片尺寸\n",
        "    batch=16,                  # 批次大小（根据显存调整）\n",
        "    patience=5,                # 5轮无改善则早停\n",
        "    save=True,                 # 保存模型\n",
        "    project='runs/train',      # 保存目录\n",
        "    name='coco128_demo',       # 实验名称\n",
        "    verbose=True               # 显示详细信息\n",
        ")\n",
        "\n",
        "print('\\n' + '='*50)\n",
        "print('          训练完成')\n",
        "print('='*50)\n",
        "# 训练结果保存在: runs/train/coco128_demo/\n",
        "# 包含:\n",
        "#  - weights/best.pt: 最佳模型权重\n",
        "#  - weights/last.pt: 最后一轮权重\n",
        "#  - results.png: 训练曲线图\n",
        "#  - confusion_matrix.png: 混淆矩阵"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.5.3 测试训练好的模型 ====================\n",
        "\n",
        "# 加载刚才训练的最佳模型\n",
        "trained_model = YOLO('runs/train/coco128_demo4/weights/best.pt')\n",
        "\n",
        "# 使用训练好的模型检测图片\n",
        "test_results = trained_model('https://ultralytics.com/images/bus.jpg', conf=0.25, verbose=False)\n",
        "\n",
        "# 打印检测结果\n",
        "print('========== 检测结果 ==========')\n",
        "boxes = test_results[0].boxes\n",
        "if len(boxes) == 0:\n",
        "    print('未检测到任何物体')\n",
        "else:\n",
        "    for i, box in enumerate(boxes):\n",
        "        cls_id = int(box.cls[0])\n",
        "        cls_name = trained_model.names[cls_id]\n",
        "        confidence = float(box.conf[0])\n",
        "        print(f'{i+1}. {cls_name}: {confidence:.2%}')\n",
        "\n",
        "# 可视化检测结果\n",
        "annotated_trained = test_results[0].plot()\n",
        "annotated_trained_rgb = cv2.cvtColor(annotated_trained, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(annotated_trained_rgb)\n",
        "plt.title('使用训练好的模型进行检测', fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5.4 YOLO数据集格式\n",
        "\n",
        "为了能够获得自己的数据集，我们需要了解YOLO的数据集格式。\n",
        "\n",
        "**标准目录结构**:\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── train/          # 训练集图片\n",
        "│   │   ├── img001.jpg\n",
        "│   │   ├── img002.jpg\n",
        "│   │   └── ...\n",
        "│   └── val/            # 验证集图片\n",
        "│       ├── img001.jpg\n",
        "│       └── ...\n",
        "├── labels/\n",
        "│   ├── train/          # 训练集标注（与图片一一对应）\n",
        "│   │   ├── img001.txt\n",
        "│   │   ├── img002.txt\n",
        "│   │   └── ...\n",
        "│   └── val/            # 验证集标注\n",
        "│       ├── img001.txt\n",
        "│       └── ...\n",
        "└── data.yaml           # 数据集配置文件\n",
        "```\n",
        "\n",
        "**标注文件格式** (每行一个物体):\n",
        "```\n",
        "class_id center_x center_y width height\n",
        "```\n",
        "\n",
        "**重要**: 所有坐标都是**归一化值** (0-1之间):\n",
        "- `center_x` = 物体中心x坐标 / 图片宽度\n",
        "- `center_y` = 物体中心y坐标 / 图片高度\n",
        "- `width` = 物体宽度 / 图片宽度\n",
        "- `height` = 物体高度 / 图片高度\n",
        "\n",
        "**标注示例** (假设图片尺寸800×600):\n",
        "```\n",
        "0 0.5 0.5 0.3 0.4    # 类别0, 中心(400,300), 宽240, 高240\n",
        "1 0.2 0.3 0.1 0.2    # 类别1, 中心(160,180), 宽80, 高120\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "yaml"
        }
      },
      "outputs": [],
      "source": [
        "# ==================== 4.5.5 data.yaml配置文件示例 ====================\n",
        "# data.yaml负责告诉YOLO数据在哪里，有哪些类别\n",
        "\n",
        "# 数据集根目录（可以是绝对路径或相对路径）\n",
        "path: ./my_dataset\n",
        "\n",
        "# 训练集和验证集的相对路径（相对于path）\n",
        "train: images/train  # 训练集图片文件夹\n",
        "val: images/val      # 验证集图片文件夹\n",
        "# 注意：labels文件夹会自动对应，无需指定\n",
        "\n",
        "# 类别数量（必须与实际类别数量一致）\n",
        "nc: 2\n",
        "\n",
        "# 类别名称（索引从0开始，用于显示检测结果）\n",
        "names:\n",
        "  0: cat\n",
        "  1: dog\n",
        "\n",
        "# 1. 将此文件保存为 data.yaml\n",
        "# 2. 确保path路径正确指向你的数据集\n",
        "# 3. train/val 是相对于path的路径\n",
        "# 4. nc 必须等于实际的类别数量\n",
        "# 5. names 中的索引要与标注文件中的class_id对应"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5.6 训练自己数据集的步骤\n",
        "\n",
        "**1: 收集图片**\n",
        "- 每个类别至少100张图片（越多越好）\n",
        "- 图片质量要好，光线、角度多样化\n",
        "- 确保包含各种场景和背景\n",
        "\n",
        "**2: 标注数据**\n",
        "可以使用标注工具，几种常见的数据集标注工具:\n",
        "- **LabelImg**: 简单易用\n",
        "- **Roboflow**: 在线标注，自动转换格式\n",
        "- **CVAT**: 功能强大，适合团队协作\n",
        "\n",
        "**3: 整理数据集**\n",
        "按照上面的目录结构组织文件，创建data.yaml\n",
        "\n",
        "**4: 开始训练**\n",
        "使用下面的训练代码模板\n",
        "\n",
        "**注：不要直接运行下面的这个代码块，你需要先完成相关的配置才能开始训练自己的数据集**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.5.7 自定义数据集训练代码模板 ====================\n",
        "\n",
        "# 训练技巧：\n",
        "# 1. 从小模型开始（yolov8n），确保流程正确\n",
        "# 2. 先用少量epochs测试（如10轮），验证数据集正确\n",
        "# 3. 监控训练曲线，避免过拟合\n",
        "# 4. 根据显存调整batch_size（8/16/32）\n",
        "# 5. 如果准确率不够，尝试更大的模型或更多数据\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ========== 1: 加载预训练模型 ==========\n",
        "model = YOLO('yolov8n.pt')  # nano版本，最小，速度快\n",
        "# 或选择其他版本：\n",
        "# model = YOLO('yolov8s.pt')  # small - 较小，速度较快\n",
        "# model = YOLO('yolov8m.pt')  # medium - 平衡性能和速度\n",
        "# model = YOLO('yolov8l.pt')  # large - 最准确但最慢\n",
        "\n",
        "# ========== 步骤2: 开始训练 ==========\n",
        "results = model.train(\n",
        "    # ===== 必需参数 =====\n",
        "    data='path/to/your/data.yaml',  # 改为你的data.yaml路径\n",
        "    \n",
        "    # ===== 训练参数 =====\n",
        "    epochs=100,              # 训练轮数（建议50-200）\n",
        "    imgsz=640,               # 输入图片尺寸（640是标准）\n",
        "    batch=16,                # 批次大小（根据显存调整：8/16/32）\n",
        "    \n",
        "    # ===== 优化参数 =====\n",
        "    patience=20,             # 早停耐心值（20轮无改善则停止）\n",
        "    lr0=0.01,                # 初始学习率\n",
        "    optimizer='Adam',        # 优化器 (Adam/SGD/AdamW)\n",
        "    \n",
        "    # ===== 数据增强 =====\n",
        "    hsv_h=0.015,            # 色调增强\n",
        "    hsv_s=0.7,              # 饱和度增强\n",
        "    hsv_v=0.4,              # 亮度增强\n",
        "    degrees=0.0,            # 旋转角度\n",
        "    translate=0.1,          # 平移\n",
        "    scale=0.5,              # 缩放\n",
        "    flipud=0.0,             # 上下翻转概率\n",
        "    fliplr=0.5,             # 左右翻转概率\n",
        "    mosaic=1.0,             # mosaic增强概率\n",
        "    \n",
        "    # ===== 保存设置 =====\n",
        "    save=True,              # 保存检查点\n",
        "    save_period=10,         # 每10轮保存一次\n",
        "    project='runs/train',   # 保存根目录\n",
        "    name='my_custom_model', # 实验名称\n",
        "    exist_ok=False,         # 如果名称存在是否覆盖\n",
        "    \n",
        "    # ===== 硬件设置 =====\n",
        "    device=0,               # GPU设备（0/1/2或'cpu'）\n",
        "    workers=8,              # 数据加载线程数\n",
        "    \n",
        "    # ===== 其他 =====\n",
        "    verbose=True,           # 显示详细信息\n",
        "    resume=False,           # 是否从上次中断处继续\n",
        ")\n",
        "\n",
        "# ========== 步骤3: 评估模型 ==========\n",
        "metrics = model.val()\n",
        "print(f'\\n评估指标：')\n",
        "print(f'mAP50: {metrics.box.map50:.3f}      # IoU阈值0.5时的平均精度')\n",
        "print(f'mAP50-95: {metrics.box.map:.3f}   # IoU从0.5到0.95的平均精度（更严格）')\n",
        "print(f'Precision: {metrics.box.mp:.3f}   # 精确率（检测到的物体中有多少是正确的）')\n",
        "print(f'Recall: {metrics.box.mr:.3f}      # 召回率（实际物体中有多少被检测到）')\n",
        "\n",
        "# ========== 步骤4: 使用训练好的模型进行推理 ==========\n",
        "best_model = YOLO('runs/train/my_custom_model/weights/best.pt')\n",
        "test_results = best_model('test_image.jpg')  # 改为你的测试图片路径\n",
        "test_results[0].show()  # 显示结果\n",
        "\n",
        "# ========== 步骤5: 导出模型（可选） ==========\n",
        "# 导出为其他格式以便部署\n",
        "best_model.export(format='onnx')   # ONNX格式，跨平台\n",
        "# best_model.export(format='torchscript')  # TorchScript\n",
        "# best_model.export(format='coreml')       # CoreML (iOS)\n",
        "# best_model.export(format='tflite')       # TensorFlow Lite (移动端)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6 YOLO其他功能演示\n",
        "\n",
        "YOLOv8不仅支持目标检测，还支持:\n",
        "- **分割 (Segmentation)**: 像素级别的物体分割\n",
        "- **姿态估计 (Pose)**: 人体关键点检测\n",
        "- **分类 (Classification)**: 图像分类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.9 实例分割演示 ====================\n",
        "\n",
        "# 加载分割模型\n",
        "seg_model = YOLO('yolov8n-seg.pt')\n",
        "\n",
        "# 对示例图片进行分割\n",
        "seg_results = seg_model('https://ultralytics.com/images/bus.jpg', verbose=False)\n",
        "\n",
        "# 可视化分割结果\n",
        "seg_img = seg_results[0].plot()\n",
        "seg_img_rgb = cv2.cvtColor(seg_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(seg_img_rgb)\n",
        "plt.title('YOLOv8实例分割结果', fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 4.10 姿态估计演示 ====================\n",
        "\n",
        "# 加载姿态估计模型\n",
        "pose_model = YOLO('yolov8n-pose.pt')\n",
        "\n",
        "# 对示例图片进行姿态估计\n",
        "pose_results = pose_model('https://ultralytics.com/images/bus.jpg', verbose=False)\n",
        "\n",
        "# 可视化姿态估计结果\n",
        "pose_img = pose_results[0].plot()\n",
        "pose_img_rgb = cv2.cvtColor(pose_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(pose_img_rgb)\n",
        "plt.title('YOLOv8姿态估计结果', fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5: 课程总结\n",
        "\n",
        "### 知识点回顾\n",
        "\n",
        "```\n",
        "MNIST数据集\n",
        "    │\n",
        "    ├── 全连接网络 (MLP)\n",
        "    │     └── 简单直接，但参数多\n",
        "    │\n",
        "    ├── 卷积神经网络 (CNN)\n",
        "    │     └── 卷积+池化，参数共享，效果更好\n",
        "    │\n",
        "    └── 残差网络 (ResNet)\n",
        "          └── 残差连接，可训练深层网络\n",
        "\n",
        "YOLO目标检测\n",
        "    │\n",
        "    ├── 图片检测 - 静态图片物体识别\n",
        "    ├── 视频/实时检测 - 摄像头实时识别\n",
        "    ├── 实例分割 - 像素级物体分割\n",
        "    └── 姿态估计 - 人体关键点检测\n",
        "```\n",
        "\n",
        "### 延伸学习资源\n",
        "\n",
        "- **PyTorch官方教程**: https://pytorch.org/tutorials/\n",
        "- **Ultralytics文档**: https://docs.ultralytics.com/\n",
        "- **动手学深度学习**: https://zh.d2l.ai/\n",
        "\n",
        "### 下一步建议\n",
        "\n",
        "1. 尝试修改网络结构，观察准确率变化\n",
        "2. 用YOLO检测自己拍摄的照片\n",
        "3. 收集数据训练自己的检测模型"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ptorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
